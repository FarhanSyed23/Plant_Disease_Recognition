{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length for each category: [100, 100, 100, 100, 100, 100]\n",
      "number of images for each category: [100, 100, 100, 100, 100, 100]\n",
      "256,256 is the min shape for Cherry___healthy\n",
      "256,256 is the min shape for Cherry_Powdery_mildew\n",
      "256,256 is the min shape for Corn___healthy\n",
      "256,256 is the min shape for Corn_Common_rust\n",
      "256,256 is the min shape for Grape___healthy\n",
      "256,256 is the min shape for Grape_Isariopsis_Leaf_Spot\n",
      "[INFO] pixels matrix: 28.80MB\n",
      "[INFO] features matrix: 9.60MB\n",
      "Random Forest Classifier: \n",
      "\n",
      "Confusion Matrix:\n",
      "[[19  4  1  0  4  0]\n",
      " [ 1 20  1  0  1  2]\n",
      " [ 0  0 27  0  0  0]\n",
      " [ 0  1  0 26  0  0]\n",
      " [ 4  1  1  0 15  0]\n",
      " [ 0  0  0  1  2 19]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cherry       0.79      0.68      0.73        28\n",
      "     CherryD       0.77      0.80      0.78        25\n",
      "        Corn       0.90      1.00      0.95        27\n",
      "       CornD       0.96      0.96      0.96        27\n",
      "       Grape       0.68      0.71      0.70        21\n",
      "      GrapeD       0.90      0.86      0.88        22\n",
      "\n",
      "    accuracy                           0.84       150\n",
      "   macro avg       0.84      0.84      0.83       150\n",
      "weighted avg       0.84      0.84      0.84       150\n",
      "\n",
      "Accuracy of Random Forest: 84.00%\n",
      "Accuracy of features :  80.0\n",
      "Decision Tree Classifier: \n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  8  0  0 10  0]\n",
      " [ 0 19  1  3  2  0]\n",
      " [ 4  3  9  7  4  0]\n",
      " [ 0  2  0 25  0  0]\n",
      " [ 3  2  1  0 14  1]\n",
      " [ 0 10  0  1  7  4]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cherry       0.59      0.36      0.44        28\n",
      "     CherryD       0.43      0.76      0.55        25\n",
      "        Corn       0.82      0.33      0.47        27\n",
      "       CornD       0.69      0.93      0.79        27\n",
      "       Grape       0.38      0.67      0.48        21\n",
      "      GrapeD       0.80      0.18      0.30        22\n",
      "\n",
      "    accuracy                           0.54       150\n",
      "   macro avg       0.62      0.54      0.51       150\n",
      "weighted avg       0.62      0.54      0.51       150\n",
      "\n",
      "Accuracy of decision tree: 54.00%\n",
      "Accuracy of features :  32.666666666666664\n",
      "Naive Bayes Classifier: \n",
      "\n",
      "Confusion Matrix:\n",
      "[[16  3  1  0  5  3]\n",
      " [ 1 21  0  1  1  1]\n",
      " [ 0  0 25  2  0  0]\n",
      " [ 0  6  0 21  0  0]\n",
      " [ 4  1  1  0 15  0]\n",
      " [ 1  3  1  0  1 16]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cherry       0.73      0.57      0.64        28\n",
      "     CherryD       0.62      0.84      0.71        25\n",
      "        Corn       0.89      0.93      0.91        27\n",
      "       CornD       0.88      0.78      0.82        27\n",
      "       Grape       0.68      0.71      0.70        21\n",
      "      GrapeD       0.80      0.73      0.76        22\n",
      "\n",
      "    accuracy                           0.76       150\n",
      "   macro avg       0.77      0.76      0.76       150\n",
      "weighted avg       0.77      0.76      0.76       150\n",
      "\n",
      "Accuracy of Naive Bayes Classifier: 76.00%\n",
      "Accuracy of features :  69.33333333333334\n",
      "SVM Classifier: \n",
      "\n",
      "Confusion Matrix:\n",
      "[[20  3  0  0  5  0]\n",
      " [ 6 15  0  1  2  1]\n",
      " [ 0  0 27  0  0  0]\n",
      " [ 0  1  0 26  0  0]\n",
      " [ 5  1  0  0 15  0]\n",
      " [ 1  1  0  0  3 17]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cherry       0.62      0.71      0.67        28\n",
      "     CherryD       0.71      0.60      0.65        25\n",
      "        Corn       1.00      1.00      1.00        27\n",
      "       CornD       0.96      0.96      0.96        27\n",
      "       Grape       0.60      0.71      0.65        21\n",
      "      GrapeD       0.94      0.77      0.85        22\n",
      "\n",
      "    accuracy                           0.80       150\n",
      "   macro avg       0.81      0.79      0.80       150\n",
      "weighted avg       0.81      0.80      0.80       150\n",
      "\n",
      "Accuracy of SVM Classifier: 80.00%\n",
      "Accuracy of features :  64.66666666666666\n",
      "xgBoost Classifier: \n",
      "\n",
      "Confusion Matrix:\n",
      "[[16  7  1  0  4  0]\n",
      " [ 1 19  0  0  2  3]\n",
      " [ 0  2 23  2  0  0]\n",
      " [ 0  3  1 21  0  2]\n",
      " [ 4  1  1  0 15  0]\n",
      " [ 0  0  1  1  2 18]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cherry       0.76      0.57      0.65        28\n",
      "     CherryD       0.59      0.76      0.67        25\n",
      "        Corn       0.85      0.85      0.85        27\n",
      "       CornD       0.88      0.78      0.82        27\n",
      "       Grape       0.65      0.71      0.68        21\n",
      "      GrapeD       0.78      0.82      0.80        22\n",
      "\n",
      "    accuracy                           0.75       150\n",
      "   macro avg       0.75      0.75      0.75       150\n",
      "weighted avg       0.76      0.75      0.75       150\n",
      "\n",
      "Accuracy of xgBoost Classifier: 74.67%\n",
      "Accuracy of features :  63.33333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Habib\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classifier: \n",
      "\n",
      "Confusion Matrix:\n",
      "[[17  1  0  0 10  0]\n",
      " [ 2 14  0  2  6  1]\n",
      " [ 3  0 12 11  1  0]\n",
      " [ 3  1  3 20  0  0]\n",
      " [ 5  1  0  0 13  2]\n",
      " [ 0  0  0  2  2 18]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cherry       0.57      0.61      0.59        28\n",
      "     CherryD       0.82      0.56      0.67        25\n",
      "        Corn       0.80      0.44      0.57        27\n",
      "       CornD       0.57      0.74      0.65        27\n",
      "       Grape       0.41      0.62      0.49        21\n",
      "      GrapeD       0.86      0.82      0.84        22\n",
      "\n",
      "    accuracy                           0.63       150\n",
      "   macro avg       0.67      0.63      0.63       150\n",
      "weighted avg       0.67      0.63      0.63       150\n",
      "\n",
      "Accuracy of Logistic Regression Classifier: 62.67%\n",
      "Accuracy of features :  68.66666666666667\n",
      "Linear Regression Classifier: \n",
      "\n",
      "Confusion Matrix:\n",
      "[[20  5  0  0  3  0]\n",
      " [ 2 21  0  1  1  0]\n",
      " [ 0  0 27  0  0  0]\n",
      " [ 0  2  0 25  0  0]\n",
      " [ 4  1  1  0 15  0]\n",
      " [ 1  1  0  1  0 19]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cherry       0.74      0.71      0.73        28\n",
      "     CherryD       0.70      0.84      0.76        25\n",
      "        Corn       0.96      1.00      0.98        27\n",
      "       CornD       0.93      0.93      0.93        27\n",
      "       Grape       0.79      0.71      0.75        21\n",
      "      GrapeD       1.00      0.86      0.93        22\n",
      "\n",
      "    accuracy                           0.85       150\n",
      "   macro avg       0.85      0.84      0.85       150\n",
      "weighted avg       0.85      0.85      0.85       150\n",
      "\n",
      "Accuracy of Linear Regression Classifier: 84.67%\n",
      "Accuracy of features :  68.66666666666667\n",
      "[INFO] evaluating raw pixel accuracy...\n",
      "\n",
      "KNN classifier ( k = 1)\n",
      "\n",
      "Accuracy of KNN: 81.3%\n",
      "Confusion Matrix:\n",
      "[[19  2  1  0  5  1]\n",
      " [ 2 19  0  3  1  0]\n",
      " [ 0  0 26  0  1  0]\n",
      " [ 1  0  0 26  0  0]\n",
      " [ 2  1  1  0 17  0]\n",
      " [ 0  0  0  1  6 15]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Cherry       0.79      0.68      0.73        28\n",
      "     CherryD       0.86      0.76      0.81        25\n",
      "        Corn       0.93      0.96      0.95        27\n",
      "       CornD       0.87      0.96      0.91        27\n",
      "       Grape       0.57      0.81      0.67        21\n",
      "      GrapeD       0.94      0.68      0.79        22\n",
      "\n",
      "    accuracy                           0.81       150\n",
      "   macro avg       0.83      0.81      0.81       150\n",
      "weighted avg       0.83      0.81      0.81       150\n",
      "\n",
      "[INFO] raw training accuracy of KNN: 100.00%\n",
      "[INFO] raw pixel testing accuracy of KNN: 81.33%\n",
      "[INFO] Features accuracy of KNN: 64.00%\n",
      "\n",
      "KNN classifier ( k = 3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Normalized Cut\n",
    "\n",
    "import cv2\n",
    "from skimage import data, segmentation, color, filters\n",
    "from skimage.future import graph\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import imutils # a simple image utility library\n",
    "import cv2 #opencv library\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "base_path = 'I:/Research/datasets/Plant_sample100'\n",
    "categories = ['Cherry___healthy', 'Cherry_Powdery_mildew', 'Corn___healthy', \n",
    "              'Corn_Common_rust', 'Grape___healthy', 'Grape_Isariopsis_Leaf_Spot']\n",
    "fnames = []\n",
    "for category in categories:\n",
    "    folder = os.path.join(base_path, category)\n",
    "    file_names = os.listdir(folder)\n",
    "    full_path = [os.path.join(folder, file_name) for file_name in file_names]\n",
    "    fnames.append(full_path)\n",
    "print('length for each category:', [len(f) for f in fnames])\n",
    "\n",
    "#reading images\n",
    "images = []\n",
    "for names in fnames:\n",
    "    one_category_images = [cv2.imread(name) for name in names if (cv2.imread(name)) is not None]\n",
    "    images.append(one_category_images)\n",
    "print('number of images for each category:', [len(f) for f in images])\n",
    "\n",
    "#identifying minimum shap for images\n",
    "for i,imgs in enumerate(images):\n",
    "    shapes = [img.shape for img in imgs]\n",
    "    widths = [shape[0] for shape in shapes]\n",
    "    heights = [shape[1] for shape in shapes]\n",
    "    print('%d,%d is the min shape for %s' % (np.min(widths), np.min(heights), categories[i]))\n",
    "\n",
    "def bgr2rgb(img):\n",
    "    return cv2.cvtColor(img.copy(), cv2.COLOR_BGR2RGB)\n",
    "# resize the image to a fixed size, then flatten the image into a list of raw pixel intensities\n",
    "def image_to_feature_vector(images, size=(128, 128)):\n",
    "    return cv2.resize(images, size).flatten()\n",
    "# extract a 3D color histogram from the HSV color space using the supplied number of bins per channel\n",
    "def extract_color_histogram(imgs, bins=(16, 16, 16)):\n",
    "    hsv = cv2.cvtColor(imgs, cv2.COLOR_BGR2RGB)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, bins,[0, 180, 0, 256, 0, 256])\n",
    "    if imutils.is_cv2():\n",
    "        hist = cv2.normalize(hist)\n",
    "    else:\n",
    "        cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "rawImages = []\n",
    "labels = []\n",
    "features = []\n",
    "\n",
    "imagePaths = list(paths.list_images(base_path))\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    \n",
    "#     fig, axes = plt.subplots(ncols=4, figsize=(15, 3.5))\n",
    "#     ax = axes.ravel()\n",
    "#     ax[0] = plt.subplot(1, 4, 1)\n",
    "#     ax[1] = plt.subplot(1, 4, 2)\n",
    "#     ax[2] = plt.subplot(1, 4, 3) \n",
    "#     ax[2] = plt.subplot(1, 4, 4) \n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    \n",
    "#     ax[0].imshow(bgr2rgb(image))\n",
    "#     ax[0].set_title('Original')\n",
    "#     ax[0].axis('off')\n",
    "    \n",
    "    label = imagePath.split(os.path.sep)[-1].split(\".\")[0]\n",
    "    \n",
    "#   Normalize Cut........................................................................\n",
    "    \n",
    "    labels1 = segmentation.slic(image, compactness=30, n_segments=400)\n",
    "    out1 = color.label2rgb(labels1, image, kind='avg')\n",
    "    g = graph.rag_mean_color(image, labels1, mode='similarity')\n",
    "    labels2 = graph.cut_normalized(labels1, g)\n",
    "    out2 = color.label2rgb(labels2, image, kind='avg')\n",
    "    \n",
    "#     ax[1].imshow(out1, cmap=plt.cm.gray)\n",
    "#     ax[1].set_title('out1')\n",
    "#     ax[1].axis('off')\n",
    "    \n",
    "#     ax[2].imshow(out2, cmap=plt.cm.gray)\n",
    "#     ax[2].set_title('out2')\n",
    "#     ax[2].axis('off')\n",
    "    \n",
    "    hist = extract_color_histogram(out2)\n",
    "    pixels = image_to_feature_vector(out2)\n",
    "    rawImages.append(pixels)  #features\n",
    "    labels.append(label)\n",
    "    features.append(hist)\n",
    "    \n",
    "#     ax[3].hist(out2.ravel(),256,[0,256], color='r')\n",
    "#     ax[3].set_title('Histogram')\n",
    "#     plt.show()\n",
    "\n",
    "rawImages = np.array(rawImages)\n",
    "labels = np.array(labels)\n",
    "features = np.array(features)\n",
    "\n",
    "print(\"[INFO] pixels matrix: {:.2f}MB\".format(rawImages.nbytes / (1024 * 1000.0)))\n",
    "print(\"[INFO] features matrix: {:.2f}MB\".format(features.nbytes / (1024 * 1000.0)))\n",
    "\n",
    "# partition the data into training and testing splits, using 75%\n",
    "# of the data for training and the remaining 25% for testing\n",
    "(trainRI, testRI, trainRL, testRL) = train_test_split(rawImages, labels, test_size=0.25, random_state=22)\n",
    "(trainFeat, testFeat, trainLabels, testLabels) = train_test_split(features, labels, test_size=0.25, random_state=22)\n",
    "\n",
    "X_train,X_test, y_train, y_test = train_test_split(rawImages, labels, test_size=0.25, random_state=22)\n",
    "(train_Feat, test_Feat, train_Labels, test_Labels) = train_test_split(features, labels, test_size=0.25, random_state=22)\n",
    "\n",
    "#Random Forest Classifier:.................................................................................................\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=90)\n",
    "clf1=RandomForestClassifier(n_estimators=90)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "clf1.fit(train_Feat,train_Labels)\n",
    "\n",
    "#Test the model\n",
    "y_pred=clf.predict(X_test)\n",
    "y_pred1=clf1.predict(test_Feat)\n",
    "\n",
    "#Generate confusion matrix\n",
    "print(\"Random Forest Classifier: \\n\")\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "\n",
    "#Generate classification report\n",
    "result1 = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(result1)\n",
    "R_acc=(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy of Random Forest: {:.2f}%\".format(R_acc * 100))\n",
    "print(\"Accuracy of features : \",(metrics.accuracy_score(test_Labels, y_pred1)*100))\n",
    "\n",
    "#Decision Tree Classifier:................................................................................................\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "Dclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "Dclf1 = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "Dclf.fit(X_train,y_train)\n",
    "Dclf1.fit(train_Feat,train_Labels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "Dy_pred = Dclf.predict(X_test)\n",
    "Dy_pred1=Dclf1.predict(test_Feat)\n",
    "print(\"Decision Tree Classifier: \\n\")\n",
    "result2 = confusion_matrix(y_test, Dy_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result2)\n",
    "\n",
    "#Generate classification report\n",
    "result3 = classification_report(y_test, Dy_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(result3)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "D_acc=metrics.accuracy_score(y_test, Dy_pred)\n",
    "print(\"Accuracy of decision tree: {:.2f}%\".format(D_acc * 100))\n",
    "print(\"Accuracy of features : \",(metrics.accuracy_score(test_Labels, Dy_pred1)*100))\n",
    "\n",
    "#Naive Bayes Classifier:................................................................................................\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "gnb = GaussianNB()\n",
    "gnb1 = GaussianNB()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "gnb.fit(X_train,y_train)\n",
    "gnb1.fit(train_Feat,train_Labels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "Ny_pred = gnb.predict(X_test)\n",
    "Ny_pred1=gnb1.predict(test_Feat)\n",
    "print(\"Naive Bayes Classifier: \\n\")\n",
    "result2 = confusion_matrix(y_test, Ny_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result2)\n",
    "\n",
    "#Generate classification report\n",
    "result3 = classification_report(y_test, Ny_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(result3)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "N_acc=metrics.accuracy_score(y_test, Ny_pred)\n",
    "print(\"Accuracy of Naive Bayes Classifier: {:.2f}%\".format(N_acc * 100))\n",
    "print(\"Accuracy of features : \",(metrics.accuracy_score(test_Labels, Ny_pred1)*100))\n",
    "\n",
    "#SVM Classifier:.................................................................................................\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "Sclf = svm.SVC(kernel='linear')\n",
    "Sclf1 = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "Sclf.fit(X_train,y_train)\n",
    "Sclf1.fit(train_Feat,train_Labels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "Sy_pred = Sclf.predict(X_test)\n",
    "Sy_pred1=Sclf1.predict(test_Feat)\n",
    "print(\"SVM Classifier: \\n\")\n",
    "result2 = confusion_matrix(y_test, Sy_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result2)\n",
    "\n",
    "#Generate classification report\n",
    "result3 = classification_report(y_test, Sy_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(result3)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "S_acc=metrics.accuracy_score(y_test, Sy_pred)\n",
    "print(\"Accuracy of SVM Classifier: {:.2f}%\".format(S_acc * 100))\n",
    "print(\"Accuracy of features : \",(metrics.accuracy_score(test_Labels, Sy_pred1)*100))\n",
    "\n",
    "#XGBoost:......................................................................................................\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "xg_cl = xgb.XGBClassifier(n_estimators=10,objective ='binary:logistic',colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10,)\n",
    "xg_cl1 = xgb.XGBClassifier(n_estimators=10,objective ='binary:logistic',colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10,)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "xg_cl.fit(X_train,y_train)\n",
    "xg_cl1.fit(train_Feat,train_Labels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "xgy_pred = xg_cl.predict(X_test)\n",
    "xgy_pred1=xg_cl1.predict(test_Feat)\n",
    "print(\"xgBoost Classifier: \\n\")\n",
    "result2 = confusion_matrix(y_test, xgy_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result2)\n",
    "\n",
    "#Generate classification report\n",
    "result3 = classification_report(y_test, xgy_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(result3)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "xg_acc=metrics.accuracy_score(y_test, xgy_pred)\n",
    "print(\"Accuracy of xgBoost Classifier: {:.2f}%\".format(xg_acc * 100))\n",
    "print(\"Accuracy of features : \",(metrics.accuracy_score(test_Labels, xgy_pred1)*100))\n",
    "\n",
    "#Logistic Regression:................................................................................................\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=1500, multi_class='auto')\n",
    "lr1 = LogisticRegression(solver='lbfgs', max_iter=1500, multi_class='auto')\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "lr.fit(X_train,y_train)\n",
    "lr1.fit(train_Feat,train_Labels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "ly_pred = lr.predict(X_test)\n",
    "ly_pred1=lr1.predict(test_Feat)\n",
    "print(\"Logistic Regression Classifier: \\n\")\n",
    "result2 = confusion_matrix(y_test, ly_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result2)\n",
    "\n",
    "#Generate classification report\n",
    "result3 = classification_report(y_test, ly_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(result3)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "l_acc=metrics.accuracy_score(y_test, ly_pred)\n",
    "print(\"Accuracy of Logistic Regression Classifier: {:.2f}%\".format(l_acc * 100))\n",
    "print(\"Accuracy of features : \",(metrics.accuracy_score(test_Labels, ly_pred1)*100))\n",
    "\n",
    "#Linear Regression with SVM:.............................................................................\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "svm = SVC(gamma='scale')\n",
    "svm1 = SVC(gamma='scale')\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "svm.fit(X_train,y_train)\n",
    "svm1.fit(train_Feat,train_Labels)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "svy_pred = svm.predict(X_test)\n",
    "svy_pred1=svm1.predict(test_Feat)\n",
    "print(\"Linear Regression Classifier: \\n\")\n",
    "result2 = confusion_matrix(y_test, svy_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result2)\n",
    "\n",
    "#Generate classification report\n",
    "result3 = classification_report(y_test, svy_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(result3)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "sv_acc=metrics.accuracy_score(y_test, svy_pred)\n",
    "print(\"Accuracy of Linear Regression Classifier: {:.2f}%\".format(sv_acc * 100))\n",
    "print(\"Accuracy of features : \",(metrics.accuracy_score(test_Labels, ly_pred1)*100))\n",
    "\n",
    "#Knn Classifier:..................................................................................................\n",
    "# train and evaluate a k-NN classifer on the raw pixel intensities\n",
    "print(\"[INFO] evaluating raw pixel accuracy...\")\n",
    "\n",
    "neighbors = [1, 3, 5, 7, 9, 13]\n",
    "train_scores=[]\n",
    "k_pred = []\n",
    "test_scores=[]\n",
    "\n",
    "for k in neighbors:\n",
    "    print(f\"\\nKNN classifier ( k = {k})\\n\")\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors= k)\n",
    "    model.fit(trainRI,trainRL)\n",
    "    train_score=model.score(trainRI,trainRL)\n",
    "    y_predi = model.predict(testRI)\n",
    "    prediction = metrics.accuracy_score(testRL, y_predi)\n",
    "    k_pred.append(prediction)\n",
    "    print(\"Accuracy of KNN: {:.1f}%\".format(prediction * 100))\n",
    "    \n",
    "    CM= confusion_matrix(testRL,y_predi)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(CM)\n",
    "    classi=classification_report(testRL,y_predi)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classi)\n",
    "    \n",
    "    test_score=model.score(testRI,testRL)\n",
    "    print(\"[INFO] raw training accuracy of KNN: {:.2f}%\".format(train_score * 100))\n",
    "    print(\"[INFO] raw pixel testing accuracy of KNN: {:.2f}%\".format(test_score * 100))\n",
    "    \n",
    "    model.fit(trainFeat,trainLabels)\n",
    "    acc4=model.score(testFeat,testLabels)\n",
    "    print(\"[INFO] Features accuracy of KNN: {:.2f}%\".format(acc4 * 100))\n",
    "        \n",
    "#  Make a plot\n",
    "height = [R_acc*100,D_acc*100, N_acc*100, S_acc*100, xg_acc*100, l_acc*100, sv_acc*100, \n",
    "          k_pred[0]*100, k_pred[1]*100, k_pred[2]*100, k_pred[3]*100, k_pred[4]*100, k_pred[5]*100]\n",
    "bars = ('Random', 'Decision tree', 'Naive Bayes','SVM', 'xGBoost', 'Logistic', 'Linear', \n",
    "        'KNN_1', 'KNN_3', 'KNN_5', 'KNN_7', 'KNN_9', 'KNN_13')\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.bar(y_pos, height, color=['black', 'red', 'green', 'blue', 'cyan', 'yellow', 'orange', 'blue', 'magenta', 'pink', 'blue', 'magenta', 'pink'])\n",
    "plt.xticks(y_pos, bars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
